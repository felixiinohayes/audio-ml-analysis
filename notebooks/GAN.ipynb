{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=16384, target_length=16384):\n",
    "\n",
    "    audio, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "\n",
    "    # Truncate or pad the audio to the target length\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    elif len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)), 'constant')\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and process the audio\n",
    "        signal, sr = librosa.load(self.file_paths[idx], sr=None)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        # Normalize the spectrogram\n",
    "        mel_spectrogram = (mel_spectrogram - mel_spectrogram.min()) / (mel_spectrogram.max() - mel_spectrogram.min())\n",
    "        mel_spectrogram = torch.tensor(mel_spectrogram, dtype=torch.float32)\n",
    "\n",
    "        # Add channel dimension for CNN\n",
    "        mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
    "\n",
    "        return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class WaveGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, output_length=16384, ngf=64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_length = output_length\n",
    "        self.ngf = ngf\n",
    "        \n",
    "        self.layer1 = nn.ConvTranspose1d(latent_dim, ngf * 16, kernel_size=25, stride=4, padding=11)\n",
    "        self.layer2 = nn.ConvTranspose1d(ngf * 16, ngf * 8, kernel_size=25, stride=4, padding=11)\n",
    "        self.layer3 = nn.ConvTranspose1d(ngf * 8, ngf * 4, kernel_size=25, stride=4, padding=11)\n",
    "        self.layer4 = nn.ConvTranspose1d(ngf * 4, ngf * 2, kernel_size=25, stride=4, padding=11)\n",
    "        self.layer5 = nn.ConvTranspose1d(ngf * 2, ngf, kernel_size=25, stride=4, padding=11)\n",
    "        self.layer6 = nn.ConvTranspose1d(ngf, 1, kernel_size=25, stride=4, padding=11)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), self.latent_dim, 1)\n",
    "        z = self.layer1(z)\n",
    "        \n",
    "        z = self.layer2(z)\n",
    "        \n",
    "        z = self.layer3(z)\n",
    "        \n",
    "        z = self.layer4(z)\n",
    "        z = self.layer5(z)\n",
    "        \n",
    "        z = self.layer6(z)\n",
    "        \n",
    "        z = self.activation(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class WaveGANDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf=64):\n",
    "        super(WaveGANDiscriminator, self).__init__()\n",
    "        self.ndf = ndf\n",
    "\n",
    "        self.layer1 = nn.Conv1d(1, ndf, 25, stride=4, padding=11)\n",
    "        self.layer2 = nn.Conv1d(ndf, ndf * 2, 25, stride=4, padding=11)\n",
    "        self.layer3 = nn.Conv1d(ndf * 2, ndf * 4, 25, stride=4, padding=11)\n",
    "        self.layer4 = nn.Conv1d(ndf * 4, ndf * 8, 25, stride=4, padding=11)\n",
    "        self.layer5 = nn.Conv1d(ndf * 8, ndf * 16, 25, stride=4, padding=11)\n",
    "        self.layer6 = nn.Conv1d(ndf * 16, 1, 25, stride=4, padding=11)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.activation(x)\n",
    "        x = torch.mean(x, dim=-1, keepdim=True)\n",
    "        x = self.activation(x)  # Apply sigmoid activation\n",
    "        x = x.view(x.size(0))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WaveGANGenerator()\n",
    "discriminator = WaveGANDiscriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr  = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=0.0005, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=0.0005, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "samples = [preprocess_audio(p) for p in Path().glob('../data/Kicks/*.wav')]\n",
    "\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(samples, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, real_audio in enumerate(dataloader):\n",
    "#     print(real_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 5\n",
    "latent_dim = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, real_audio in enumerate(dataloader):\n",
    "        # ---------------------\n",
    "        # (1) Train Discriminator\n",
    "        # ---------------------\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Real audio: Ensure it's shaped (batch_size, 1, length)\n",
    "        real_audio = real_audio.to(device)\n",
    "        real_audio = real_audio.unsqueeze(1)  # Add channel dimension to make it (batch_size, 1, length)\n",
    "\n",
    "        # Set label size dynamically based on the batch size\n",
    "        batch_size = real_audio.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device).float()  # Label should match batch size\n",
    "        \n",
    "        # Forward pass real audio through the discriminator\n",
    "        # if(i==0): print(real_audio.shape)\n",
    "        output = discriminator(real_audio)  # Expecting (batch_size,) output\n",
    "        # if(i==0): print(output.size(), label.size(), real_audio.size(0))\n",
    "        lossD_real = criterion(output, label)\n",
    "        lossD_real.backward()\n",
    "\n",
    "        # Generate fake audio and ensure it's (batch_size, 1, length)\n",
    "        noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        fake_audio = generator(noise)\n",
    "        if fake_audio.dim() == 2:\n",
    "            fake_audio = fake_audio.unsqueeze(1)  # Add channel dimension to make it (batch_size, 1, length)\n",
    "\n",
    "        label.fill_(fake_label)  # Fill the label for fake data to match batch size\n",
    "        output = discriminator(fake_audio.detach()).view(-1)\n",
    "        lossD_fake = criterion(output, label)\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        # Update the discriminator\n",
    "        optimizerD.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # (2) Train Generator\n",
    "        # ---------------------\n",
    "        generator.zero_grad()\n",
    "        label.fill_(real_label)  # Generator wants discriminator to classify fake as real\n",
    "        output = discriminator(fake_audio).view(-1)\n",
    "        lossG = criterion(output, label)\n",
    "        lossG.backward()\n",
    "\n",
    "        # Update the generator\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Print loss stats every few iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
    "                  f\"Loss_D: {lossD_real + lossD_fake:.4f} Loss_G: {lossG:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Generate a fake audio sample\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(1, latent_dim, device=device)  # Generate one latent vector\n",
    "    generated_audio = generator(noise).cpu().numpy()\n",
    "\n",
    "# Save the generated audio to a file\n",
    "sf.write('generated_audio.wav', generated_audio.squeeze(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
